{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "449cb23b-b85c-4838-8494-4f623d729230",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "NVIDIA GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "93165a3f-d4e0-412b-923a-d1597e846012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is GPU available? True\n",
      "GPU Name: NVIDIA GeForce MX150\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"Is GPU available?\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb0ad9ca-c39c-40ec-ab47-b5cc26d56c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    \n",
    "    # --- MX150 SAFE SETTINGS ---\n",
    "    per_device_train_batch_size=2,   # Very small batch for 4GB RAM\n",
    "    gradient_accumulation_steps=4,   # This makes the \"real\" batch size 8\n",
    "    fp16=True,                       # Reduces memory usage\n",
    "    max_steps=1000,                  # Start small to test\n",
    "    # ---------------------------\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2ce846af-017c-45f5-94e6-26b187c3e325",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['tweet_id', 'text', 'Violenc', 'Hate', 'Vulgar', 'HateSpeech'],\n",
      "        num_rows: 3528\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['tweet_id', 'text', 'Violenc', 'Hate', 'Vulgar', 'HateSpeech'],\n",
      "        num_rows: 706\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['tweet_id', 'text', 'Violenc', 'Hate', 'Vulgar', 'HateSpeech'],\n",
      "        num_rows: 2822\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Define the file paths (ensure these files are in the same folder as your notebook)\n",
    "data_files = {\n",
    "    \"train\": \"train_simple.csv\",\n",
    "    \"validation\": \"val_simple.csv\",\n",
    "    \"test\": \"test_simple.csv\"\n",
    "}\n",
    "\n",
    "# Load the dataset\n",
    "raw_datasets = load_dataset(\"csv\", data_files=data_files)\n",
    "\n",
    "# Print the structure to see columns and row counts\n",
    "print(raw_datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d976ce03-e082-4095-9f9e-4837b0758654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example row: {'tweet_id': 1222188705437429762, 'text': 'در جریانید که حق طلاق توی ایران با مرده دیگه؟؟؟', 'Violenc': 0, 'Hate': 0, 'Vulgar': 0, 'HateSpeech': 0}\n",
      "Column names: ['tweet_id', 'text', 'Violenc', 'Hate', 'Vulgar', 'HateSpeech']\n"
     ]
    }
   ],
   "source": [
    "print(\"Example row:\", raw_datasets[\"train\"][0])\n",
    "print(\"Column names:\", raw_datasets[\"train\"].column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "611e5b03-ba4b-4b73-aebc-8e4b95dfa13f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1230c9396047efa21d4effb915392d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbf8b523edf3459da88f5e1bc1a6baa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b2d6437b4fc43eaa67255ae027969a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Rename 'HateSpeech' to 'label' for the trainer\n",
    "def prepare_labels(batch):\n",
    "    batch[\"label\"] = batch[\"HateSpeech\"]\n",
    "    return batch\n",
    "\n",
    "dataset = raw_datasets.map(prepare_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3807231b-3517-4cf1-b1d8-6609a538f804",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbbba3fdd52846eb8cf631b0f9e0e32e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/292 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmirHossein\\ml-envs\\bert-gpu\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AmirHossein\\.cache\\huggingface\\hub\\models--HooshvareLab--bert-fa-zwnj-base. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50cee583621b45d9a2f813cd55a313d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/565 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a416ab495110408eb20d232c6ccfeaa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba55126a8a604369acfc8ef80144273d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "714a2ecea5c142948042ef1c35668ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/134 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10bef81a288040a59c1fe161e74cac0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b34c192695c242b8876213d76a64c4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b54c70be5e1c450ab03b8e3ca01b1f5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"HooshvareLab/bert-fa-zwnj-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3140ddec-b8ec-4962-9c54-16dcc4621dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b0e314adcc744fd804f9b52030502ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daca1ca50ad1451f991397f722eec9d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/HooshvareLab/bert-fa-zwnj-base/resolve/refs%2Fpr%2F1/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 55c260a1-3d5f-437e-9ca4-b3ad00be3ba9)')' thrown while requesting GET https://huggingface.co/HooshvareLab/bert-fa-zwnj-base/resolve/refs%2Fpr%2F1/model.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b86ab0ff2045d3b08bb76c69f33576",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   9%|8         | 41.9M/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/HooshvareLab/bert-fa-zwnj-base/resolve/refs%2Fpr%2F1/model.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n",
      "'(ReadTimeoutError(\"HTTPSConnectionPool(host='huggingface.co', port=443): Read timed out. (read timeout=10)\"), '(Request ID: 452324c1-a34d-460f-8bd6-0fbf3337a52b)')' thrown while requesting GET https://huggingface.co/HooshvareLab/bert-fa-zwnj-base/resolve/refs%2Fpr%2F1/model.safetensors\n",
      "Retrying in 1s [Retry 1/5].\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69bf52f69774c44a4b49c7afd1faedd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:  93%|#########3| 440M/473M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# We have 2 classes: 0 (Normal) and 1 (HateSpeech)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "print(f\"Model loaded on: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36e7ad9d-40df-4d0b-85ce-1b23ca4d6b27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faabe918c0b24719814a37242e0b2aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "metric = evaluate.load(\"f1\")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1b7676d2-0b33-4cff-9d75-6969ecd72add",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmirHossein\\AppData\\Local\\Temp\\ipykernel_7564\\3971506822.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bert-hate-speech-results\",\n",
    "    logging_dir=\"./logs\",\n",
    "    \n",
    "    # --- GPU MEMORY OPTIMIZATION ---\n",
    "    fp16=True,                          \n",
    "    per_device_train_batch_size=4,      \n",
    "    per_device_eval_batch_size=4,\n",
    "    gradient_accumulation_steps=4,      \n",
    "    gradient_checkpointing=True,        \n",
    "    # -------------------------------\n",
    "\n",
    "    # CHANGE: 'evaluation_strategy' becomes 'eval_strategy'\n",
    "    eval_strategy=\"epoch\",  \n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\" \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8bcbaea5-f629-4826-9557-e712b4d3aeb6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='663' max='663' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [663/663 34:09, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.486618</td>\n",
       "      <td>0.731861</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.470930</td>\n",
       "      <td>0.750392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.455700</td>\n",
       "      <td>0.555631</td>\n",
       "      <td>0.747182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=663, training_loss=0.39524363176855026, metrics={'train_runtime': 2053.2548, 'train_samples_per_second': 5.155, 'train_steps_per_second': 0.323, 'total_flos': 696191852482560.0, 'train_loss': 0.39524363176855026, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f990a0ae-5ed5-4167-ab97-3938c5d353c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Results: {'eval_loss': 0.47786349058151245, 'eval_f1': 0.7526273241713823, 'eval_runtime': 123.552, 'eval_samples_per_second': 22.841, 'eval_steps_per_second': 5.714, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate(tokenized_datasets[\"test\"])\n",
    "print(\"Test Results:\", results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b60b279-a907-40cf-8f1b-337603024cbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d0e74a34-af2e-45a7-84c5-0de3a7c41e72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.80      0.82      0.81      1563\n",
      "  HateSpeech       0.77      0.74      0.75      1259\n",
      "\n",
      "    accuracy                           0.78      2822\n",
      "   macro avg       0.78      0.78      0.78      2822\n",
      "weighted avg       0.78      0.78      0.78      2822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "\n",
    "# 1. Get predictions on the test set\n",
    "test_predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "preds = np.argmax(test_predictions.predictions, axis=-1)\n",
    "actual_labels = test_predictions.label_ids\n",
    "\n",
    "# 2. Print the detailed report\n",
    "# Label 0 = Normal, Label 1 = HateSpeech\n",
    "print(classification_report(actual_labels, preds, target_names=[\"Normal\", \"HateSpeech\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "646fdea2-1352-456b-8796-8c6e299d1282",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'trainer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model_save_path = \u001b[33m\"\u001b[39m\u001b[33m./parsbert-hate-speech-model\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Save the model and the tokenizer\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mtrainer\u001b[49m.save_model(model_save_path)\n\u001b[32m      6\u001b[39m tokenizer.save_pretrained(model_save_path)\n\u001b[32m      8\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel successfully saved to: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel_save_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNameError\u001b[39m: name 'trainer' is not defined"
     ]
    }
   ],
   "source": [
    "# Create a folder for your saved model\n",
    "model_save_path = \"./parsbert-hate-speech-model\"\n",
    "\n",
    "# Save the model and the tokenizer\n",
    "trainer.save_model(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "print(f\"Model successfully saved to: {model_save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d698e23-8397-476e-8abd-ad5c6d8f1458",
   "metadata": {},
   "source": [
    "# without training classification report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55ce0132-0828-4414-8414-cde1f723f22e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at HooshvareLab/bert-fa-zwnj-base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAW model inference (Expect random results)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2822/2822 [02:30<00:00, 18.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "REPORT FOR RAW MODEL: HooshvareLab/bert-fa-zwnj-base\n",
      "=======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.56      0.28      0.38      1563\n",
      "  HateSpeech       0.45      0.72      0.55      1259\n",
      "\n",
      "    accuracy                           0.48      2822\n",
      "   macro avg       0.50      0.50      0.46      2822\n",
      "weighted avg       0.51      0.48      0.45      2822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. Use the RAW ParsBERT checkpoint from Hugging Face\n",
    "model_checkpoint = \"HooshvareLab/bert-fa-zwnj-base\" \n",
    "\n",
    "# 2. Load Tokenizer and Model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# When we set num_labels=2, Hugging Face adds a RANDOM 2-class head on top\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "# Move to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 3. Load the Test Dataset\n",
    "dataset = load_dataset(\"csv\", data_files={\"test\": \"test_simple.csv\"})[\"test\"]\n",
    "\n",
    "# 4. Inference Loop\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Running RAW model inference (Expect random results)...\")\n",
    "for item in tqdm(dataset):\n",
    "    text = item['text']\n",
    "    label = item['HateSpeech']\n",
    "    \n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    all_preds.append(prediction)\n",
    "    all_labels.append(label)\n",
    "\n",
    "# 5. Print the Report\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(f\"REPORT FOR RAW MODEL: {model_checkpoint}\")\n",
    "print(\"=\"*55)\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"Normal\", \"HateSpeech\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4b3c53-e925-4a71-af0a-b014e98a163e",
   "metadata": {},
   "source": [
    "# AS an other model we consider DistilGPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c01b58-54ec-438a-9686-d3148c582879",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "efd3bdc8-bfb7-45b2-a0b2-c2979c615dc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72a5f3b10ba144afad20e53eeb5947d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmirHossein\\ml-envs\\bert-gpu\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\AmirHossein\\.cache\\huggingface\\hub\\models--distilgpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1535980b7315442d8b933c0e6227d844",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/762 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea57b332750f4a679a4f969b6c9c45e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "940943e71d0643d68799289092c84b0e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70732aa57665462886611acc31f8a9f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c21b7831dc974871b34c9f619eeb1e2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3528 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d1d78202256c403c881fba09edd7e854",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/706 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2f27cfe403843c4ad542dbecf9ea23e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/2822 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# CRITICAL FIX FOR GPT-2:\n",
    "# GPT-2 does not have a pad token. we use the EOS token instead.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=128)\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f7c1340-1381-4a23-809a-df2a77cbd07a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7f9303af4fd43d788f57a66ad86ae4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/353M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPT2ForSequenceClassification(\n",
       "  (transformer): GPT2Model(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.1, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-5): 6 x GPT2Block(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPT2Attention(\n",
       "          (c_attn): Conv1D(nf=2304, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=768)\n",
       "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPT2MLP(\n",
       "          (c_fc): Conv1D(nf=3072, nx=768)\n",
       "          (c_proj): Conv1D(nf=768, nx=3072)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=2, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "import torch\n",
    "\n",
    "# Load DistilGPT-2 for classification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "\n",
    "# FIX: Tell the model which ID is the padding token\n",
    "model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6db4d6f5-c752-44c5-b4cf-d1604ab78762",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmirHossein\\AppData\\Local\\Temp\\ipykernel_7564\\4077587325.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='663' max='663' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [663/663 15:31, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.687172</td>\n",
       "      <td>0.651835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.666838</td>\n",
       "      <td>0.598916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.699400</td>\n",
       "      <td>0.666499</td>\n",
       "      <td>0.534591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=663, training_loss=0.6882548727780625, metrics={'train_runtime': 933.5562, 'train_samples_per_second': 11.337, 'train_steps_per_second': 0.71, 'total_flos': 345708086427648.0, 'train_loss': 0.6882548727780625, 'epoch': 3.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./distilgpt2-hate-speech\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    \n",
    "    # GPU Optimizations\n",
    "    fp16=True,\n",
    "    per_device_train_batch_size=8,      # You can increase to 8 for DistilGPT!\n",
    "    gradient_accumulation_steps=2,\n",
    "    \n",
    "    learning_rate=5e-5,                 # GPT models often need a slightly higher LR\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics, # Use the same function as before\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c1ae2afe-a27d-43a9-9131-f27385d5924a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification Report for ParsBERT on Test Dataset:\n",
      "------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.62      0.62      0.62      1563\n",
      "  HateSpeech       0.53      0.53      0.53      1259\n",
      "\n",
      "    accuracy                           0.58      2822\n",
      "   macro avg       0.57      0.57      0.57      2822\n",
      "weighted avg       0.58      0.58      0.58      2822\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# 1. Use the trainer to predict labels for the test set\n",
    "test_output = trainer.predict(tokenized_datasets[\"test\"])\n",
    "\n",
    "# 2. Extract the predictions (logits) and true labels\n",
    "# 'test_output.predictions' are the raw scores, we use argmax to get 0 or 1\n",
    "test_preds = np.argmax(test_output.predictions, axis=-1)\n",
    "test_labels = test_output.label_ids\n",
    "\n",
    "# 3. Print the report\n",
    "print(\"Classification Report for ParsBERT on Test Dataset:\")\n",
    "print(\"-\" * 60)\n",
    "print(classification_report(test_labels, test_preds, target_names=[\"Normal\", \"HateSpeech\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "61e81cc1-f2a4-4a1d-bdac-82788c039319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "# # Create the matrix\n",
    "# cm = confusion_matrix(test_labels, test_preds)\n",
    "\n",
    "# # Plot it\n",
    "# plt.figure(figsize=(8, 6))\n",
    "# sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "#             xticklabels=[\"Normal\", \"HateSpeech\"], \n",
    "#             yticklabels=[\"Normal\", \"HateSpeech\"])\n",
    "# plt.xlabel('Predicted Label')\n",
    "# plt.ylabel('True Label')\n",
    "# plt.title('Confusion Matrix: ParsBERT Prediction')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6fcda33-4b17-4435-a75a-32b1da45f60e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./my_saved_DistilGPT-2_model\\\\tokenizer_config.json',\n",
       " './my_saved_DistilGPT-2_model\\\\special_tokens_map.json',\n",
       " './my_saved_DistilGPT-2_model\\\\vocab.json',\n",
       " './my_saved_DistilGPT-2_model\\\\merges.txt',\n",
       " './my_saved_DistilGPT-2_model\\\\added_tokens.json',\n",
       " './my_saved_DistilGPT-2_model\\\\tokenizer.json')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save weights\n",
    "model.save_pretrained(\"./my_saved_DistilGPT-2_model\")\n",
    "\n",
    "# Save tokenizer (CRITICAL: this keeps your Persian vocabulary settings)\n",
    "tokenizer.save_pretrained(\"./my_saved_DistilGPT-2_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334c593c-284f-47b3-9091-06993ceb14d9",
   "metadata": {},
   "source": [
    "# raw model of DistilGPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6b1f2d9-7e75-4e4f-ba45-f08629bc9dbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running RAW DistilGPT-2 inference (English model on Persian text)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 2822/2822 [01:18<00:00, 36.09it/s]\n",
      "C:\\Users\\AmirHossein\\ml-envs\\bert-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n",
      "C:\\Users\\AmirHossein\\ml-envs\\bert-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=======================================================\n",
      "REPORT FOR RAW MODEL: distilgpt2\n",
      "=======================================================\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.00      0.00      0.00      1563\n",
      "  HateSpeech       0.45      1.00      0.62      1259\n",
      "\n",
      "    accuracy                           0.45      2822\n",
      "   macro avg       0.22      0.50      0.31      2822\n",
      "weighted avg       0.20      0.45      0.28      2822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AmirHossein\\ml-envs\\bert-gpu\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1833: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", result.shape[0])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "# 1. استفاده از نسخه خام و انگلیسی DistilGPT-2\n",
    "model_checkpoint = \"distilgpt2\" \n",
    "\n",
    "# 2. بارگذاری توکنایزر و تنظیم توکن Pad (حیاتی برای GPT-2)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 3. بارگذاری مدل با لایه طبقه‌بندی تصادفی (Newly Initialized)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# انتقال به GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# 4. بارگذاری مجموعه داده تست\n",
    "dataset = load_dataset(\"csv\", data_files={\"test\": \"test_simple.csv\"})[\"test\"]\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "print(\"Running RAW DistilGPT-2 inference (English model on Persian text)...\")\n",
    "for item in tqdm(dataset):\n",
    "    text = item['text']\n",
    "    label = item['HateSpeech']\n",
    "    \n",
    "    # توکنایز کردن (GPT-2 از سمت چپ یا راست پدینگ می‌کند، پیش‌فرض Trainer معمولا راست است)\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, padding=\"max_length\", max_length=128).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    \n",
    "    prediction = torch.argmax(logits, dim=-1).item()\n",
    "    \n",
    "    all_preds.append(prediction)\n",
    "    all_labels.append(label)\n",
    "\n",
    "# 5. چاپ گزارش عملکرد\n",
    "print(\"\\n\" + \"=\"*55)\n",
    "print(f\"REPORT FOR RAW MODEL: {model_checkpoint}\")\n",
    "print(\"=\"*55)\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"Normal\", \"HateSpeech\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51d449a-88e7-4de6-bb2d-a346e33ff372",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17ff6856-eff6-4adb-8fa1-25b65922a462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPT2ForSequenceClassification were not initialized from the model checkpoint at distilgpt2 and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running batched inference...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 89/89 [01:03<00:00,  1.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pred counts: [ 691 2131]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      Normal       0.63      0.28      0.39      1563\n",
      "  HateSpeech       0.47      0.80      0.59      1259\n",
      "\n",
      "    accuracy                           0.51      2822\n",
      "   macro avg       0.55      0.54      0.49      2822\n",
      "weighted avg       0.56      0.51      0.48      2822\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import classification_report\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "model_checkpoint = \"distilgpt2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "tokenizer.padding_side = \"left\"   # important for GPT-2\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=2)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "dataset = load_dataset(\"csv\", data_files={\"test\": \"test_simple.csv\"})[\"test\"]\n",
    "\n",
    "# (Optional) shuffle dataset order\n",
    "dataset = dataset.shuffle(seed=42)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    texts = [x[\"text\"] for x in batch]\n",
    "    labels = torch.tensor([x[\"HateSpeech\"] for x in batch], dtype=torch.long)\n",
    "    enc = tokenizer(\n",
    "        texts,\n",
    "        truncation=True,\n",
    "        padding=True,        # dynamic padding per batch\n",
    "        max_length=128,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    enc[\"labels\"] = labels\n",
    "    return enc\n",
    "\n",
    "batch_size = 32\n",
    "loader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "all_preds, all_labels = [], []\n",
    "\n",
    "print(\"Running batched inference...\")\n",
    "for batch in tqdm(loader):\n",
    "    labels = batch.pop(\"labels\")\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        logits = model(**batch).logits\n",
    "\n",
    "    preds = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    all_preds.extend(preds.cpu().numpy().tolist())\n",
    "    all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "print(\"Pred counts:\", np.bincount(np.array(all_preds)))\n",
    "print(classification_report(all_labels, all_preds, target_names=[\"Normal\", \"HateSpeech\"], zero_division=0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc8af41-0ba6-4e33-8a6e-352418d1e4d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (bert-gpu)",
   "language": "python",
   "name": "bert-gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
